//-- ########################################################### --//
//-- ****     Prerequisites:                                **** --//
//-- ########################################################### --//

1. Docker Desktop installed     
2. Docker is up and running


//-- ########################################################### --//
       Cleaning data:
         1. Drop rows
         2. Drops columns
         3. Creating columns
         4. Modifying columns
         5. Enriching data
         6. Duplication                   
//-- ########################################################### --//


//-- ########################################################### --//
     1. Drop columns            
//-- ########################################################### --//

//-- # drop a column
df.drop(columns=['region_id'], inplace=True)


//-- ########################################################### --//
     2. Drop rows            
//-- ########################################################### --//

//-- # drop a row
df.drop(index=[34225],inplace=True)


//-- # dropna(): [axis, how, thresh, subset, inplace]
     - axis: specifies rows or columns with indexes or columns (0 or 1). It defaults to rows.
     - how: specifies whether to drop rows or columns if all the values are null or if any value is null (all or any). It defaults to any.
     - thresh: allows more control than allowing you to specify an integer value of how many nulls must be present.
     - subset: allows you to specify a list of rows or columns to search.
     - inplace: allows you to modify the existing DataFrame. It defaults to False.

//-- # find result with null value
df['start_location_name'][(df['start_location_name'].isnull())]

//-- # dropna on axis=0 with how=any - the defaults
df.dropna(subset=['start_location_name'], inplace=True)

//-- # verify search result is empty
df['start_location_name'][(df['start_location_name'].isnull())]


//-- ########################################################### --//
     3. Creating columns       
//-- ########################################################### --//

toDo


//-- ########################################################### --//
     4. Modifying columns: 
        01. upper()
        02. lower()
        03. capitalize()     
//-- ########################################################### --//

//-- # modify a specific column name
df.rename(columns={'DURATION':'duration'},inplace=True)
df.columns

//-- # modify multiple columns name
df.rename(columns={'DURATION':'duration','region_id':'region'},inplace=True)

//-- # modify all column to lower case
df.columns=[x.lower() for x in df.columns] 
print(df.columns)

//-- # modify all column to upper case
df.columns=[x.upper() for x in df.columns] 
print(df.columns)

//-- # modify values of a column
df['month'].head()
df['month']=df['month'].str.upper()
df['month'].head()


//-- ########################################################### --//
     5. Enriching data                   
//-- ########################################################### --//

//-- # e-scooter data is geographic data - it contains locations - but it lacks coordinates

//-- # take first 5 location data
new=pd.DataFrame(df['start_location_name'].value_counts().head())
new

new.reset_index(inplace=True)
new.columns=['address','count']
new

//-- # replace "@" with "and"
     # add extra column with street information
n=new['address'].str.split(pat=',',n=1,expand=True)
replaced=n[0].str.replace("@","and")
new['street']=n[0]
new['street']=replaced
new

//-- # read another file: 
geo=pd.read_csv('/home/jovyan/work/geocodedstreet.csv')
geo

//-- # left join - enrich the original DataFrame with this new data
joined=new.join(other=geo, how='left', lsuffix='_new', rsuffix='_geo')
joined

//-- # slicing
joined[['street_new','street_geo','x','y']]

//-- # merge 
merged=pd.merge(new,geo,on='street')
merged.columns


//-- ########################################################### --//
     6. Duplication                   
//-- ########################################################### --//

//-- # check duplicates


//-- # remove duplicates
df_dedup=df_dup.drop_duplicates()


//-- ########################################################### --//
     7. Missing values                   
//-- ########################################################### --//

//-- # 0. why handling missing values
- Data information loss
- Lead to wrong prediction/classification 
- sklearn implementations don't support data with missing values





//-- ##########################   END  ######################### --//    



